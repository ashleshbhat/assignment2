# Big Data Homework 2
by Tobias Joschko and Ashlesh Bhat

For the Homework 2 we created 3 different programs:
- webReader.py which can read a defined number of web pages to create large enough test data sets
- mapReducePython.py which is the sequentiell python version of map reduce in python together with the HW1 wordcounter for time comparison
- SparkWordCounter.py: Spark map/reduce program for word counting

## webReader.py

## mapReducePython.py
('took', 815), ('smith', 947), ('man', 965), ('time', 1145), ('mr', 1216), ('john', 1258), ('house', 1265), ('came', 1280), ('went', 1460), ('prisoner', 1597), ('said', 2148)

time used by map reduce:  73.68411421775818

said 2148
prisoner 1597
went 1460
came 1280
house 1265
john 1258
mr 1216
time 1145
man 965
smith 947
took 815

Dict processing time: 635.646905


## SparkWordCounter.py:


## Apache Spark

Spark is a fast and general cluster computing system for Big Data. It provides
high-level APIs in Scala, Java, Python, and R, and an optimized engine that
supports general computation graphs for data analysis. It also supports a
rich set of higher-level tools including Spark SQL for SQL and DataFrames,
MLlib for machine learning, GraphX for graph processing,
and Spark Streaming for stream processing.

<http://spark.apache.org/>


## Online Documentation

You can find the latest Spark documentation, including a programming
guide, on the [project web page](http://spark.apache.org/documentation.html).
This README file only contains basic setup instructions.



